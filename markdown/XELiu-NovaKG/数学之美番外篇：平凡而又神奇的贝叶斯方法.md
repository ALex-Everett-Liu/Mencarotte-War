- 概率论只不过是把常识[用数学公式表达了出来]。
——拉普拉斯
- ## **0. 前言**
    - 这是一篇关于[贝叶斯方法]([[Bayesian method]])的科普文，我会尽量少用公式，多用平白的语言叙述，多举实际例子。[更严格的公式和计算][我会在相应的地方注明参考资料]。贝叶斯方法被证明是非常 general 且强大的[推理框架]，文中你会看到很多有趣的应用。
- ## **1. 历史**
    - 托马斯·贝叶斯（Thomas Bayes）同学的详细生平在[这里](http://en.wikipedia.org/wiki/Thomas_Bayes)。以下摘一段 wikipedia 上的简介：
        - 所谓的贝叶斯方法源于[他生前]为解决一个“逆概”问题写的一篇文章，而这篇文章是在他死后[才由他的一位朋友发表出来的]。在贝叶斯写这篇文章之前，人们已经能够计算“正向概率”，如“假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率是多大”。而一个自然而然的问题是反过来：“[如果我们事先并不知道][袋子里面黑白球的比例]，而是[闭着眼睛摸出一个]（或好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例[作出什么样的推测]”。这个问题，就是所谓的[逆概问题]([[inverse probability]])。
    - 实际上，贝叶斯当时的论文只是对这个问题的[一个直接的求解尝试]，[并不清楚他当时是不是已经意识到][这里面包含着的深刻的思想]。然而后来，贝叶斯方法席卷了概率论，并将应用[延伸到各个问题领域]，[所有需要作出概率预测的地方][都可以见到贝叶斯方法的影子]，特别地，贝叶斯是[机器学习]([[machine learning]])的核心方法之一。这背后的深刻原因在于，[现实世界][本身就是不确定的]，[人类的观察能力][是有局限性的]（否则有很大一部分科学[就没有必要做了]——[设想我们能够直接观察到]电子的运行，还需要对原子模型[争吵不休]吗？），[我们日常所观察到的][只是事物表面上的结果]，沿用刚才那个袋子里面取球的比方，[我们往往只能知道][从里面取出来的球]是什么颜色，而[并不能直接看到][袋子里面实际的情况]。
        - 这个时候，我们就需要提供一个猜测（[[hypothesis]]，更为严格的说法是“假设”，这里用“猜测”更通俗易懂一点），所谓猜测，当然就是不确定的（[很可能有好多种乃至无数种]猜测[都能满足目前的观测]），**但也绝对不是[两眼一抹黑瞎蒙]——具体地说，我们需要做两件事情：1. 算出各种不同猜测的[可能性大小]。2. 算出[最靠谱的猜测]是什么。** 第一个就是[计算特定猜测的后验概率]，对于[连续的猜测空间]则是计算猜测的[概率密度函数]。第二个则是所谓的[模型比较]，模型比较如果[不考虑先验概率]的话就是[最大似然方法]([[maximum likelihood method]])。
220824-15:15
    - ### **1.1 一个例子：自然语言的二义性**
        - 下面举一个[自然语言的不确定性]的例子。当你看到这句话：
            - The girl saw the boy with a telescope.
            - 你对这句话的含义有什么猜测？平常人肯定会说：那个女孩拿望远镜看见了那个男孩（即你对[这个句子背后的实际语法结构]的猜测是：The girl saw-with-a-telescope the boy ）。然而，仔细一想，你会发现这个句子[完全可以解释成]：那个女孩看见了那个拿着望远镜的男孩（即：The girl saw the-boy-with-a-telescope ）。那为什么平常生活中[我们每个人都能够迅速地][对这种二义性进行消解]呢？[这背后到底隐藏着什么样的][思维法则]？我们留到后面解释。 #ambiguity
    - ### **1.2 贝叶斯公式**
        - 贝叶斯公式是怎么来的？
我们还是使用 wikipedia 上的一个例子：
            - 一所学校里面有 60% 的男生，40% 的女生。男生[总是穿长裤]，女生则[一半穿长裤一半穿裙子]。[有了这些信息之后][我们可以容易地计算]“[随机选取一个]学生，他（她）穿长裤的概率和穿裙子的概率是多大”，这个就是前面说的“正向概率”的计算。然而，假设你[走在校园中]，[迎面走来一个]穿长裤的学生（很不幸的是你高度近视，你只看得见他（她）穿的是否长裤，而无法确定他（她）的性别），你能够推断出他（她）是男生的概率是多大吗？
                - 一些[认知科学]的研究表明（《决策与判断》以及《[Rationality for Mortals](http://www.douban.com/subject/3199621/)》第12章：[小孩也可以解决]贝叶斯问题），我们对[形式化的贝叶斯问题]不擅长，但对于[以频率形式呈现的等价问题]却很擅长。在这里，我们不妨把问题重新叙述成：你在校园里面[随机游走]([[random walk]])，遇到了 N 个穿长裤的人（仍然假设你[无法直接观察到]他们的性别），问这 N 个人里面有多少个女生多少个男生。
                    - 你说，这还不简单：算出学校里面有多少穿长裤的，然后在这些人里面再算出有多少女生，不就行了？
                    - 我们来算一算：假设学校里面人的总数是 U 个。60% 的男生都穿长裤，于是我们得到了 U * P(Boy) * P(Pants|Boy) 个穿长裤的（男生）（其中 P(Boy) 是男生的概率 = 60%，这里可以简单的理解为男生的比例；P(Pants|Boy) 是条件概率，即在 Boy 这个条件下穿长裤的概率是多大，这里是 100% ，因为所有男生都穿长裤）。40% 的女生里面又有一半（50%）是穿长裤的，于是我们又得到了 U * P(Girl) * P(Pants|Girl) 个穿长裤的（女生）。加起来一共是 U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl) 个穿长裤的，其中有 U * P(Girl) * P(Pants|Girl) 个女生。两者一比就是你要求的答案。
                        - 下面我们把这个答案形式化一下：我们要求的是 P(Girl|Pants) （穿长裤的人里面有多少女生），我们计算的结果是 U * P(Girl) * P(Pants|Girl) / [U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl)] 。容易发现这里校园内人的总数是无关的，可以消去。于是得到
**P(Girl|Pants) = P(Girl) * P(Pants|Girl) / [P(Boy) * P(Pants|Boy) + P(Girl) * P(Pants|Girl)]**
                            - 注意，如果把上式收缩起来，分母其实就是 P(Pants) ，分子其实就是 P(Pants, Girl) 。而这个比例很自然地就读作：在穿长裤的人（ P(Pants) ）里面有多少（穿长裤）的女孩（ P(Pants, Girl) ）。
                            - 上式中的 Pants 和 Boy/Girl 可以指代一切东西，所以其一般形式就是：
**P(B|A) = P(A|B) * P(B) / [P(A|B) * P(B) + P(A|~B) * P(~B) ]**
                                - 收缩起来就是：
**P(B|A) = P(AB) / P(A)**
                                - 其实这个就等于：
**P(B|A) * P(A) = P(AB)**
                                - 难怪拉普拉斯说**概率论只是把常识用数学公式表达了出来**。
然而，后面我们会逐渐发现，[看似这么平凡的]贝叶斯公式，[背后却隐含着][非常深刻的原理]。
- ## **2. 拼写纠正**
    - 经典著作《人工智能：现代方法》的作者之一 Peter Norvig 曾经写过一篇介绍如何写一个[拼写检查/纠正器]的文章（原文在[这里](http://norvig.com/spell-correct.html)，徐宥的翻译版在[这里](http://blog.youxu.info/spell-correct.html)，这篇文章很深入浅出，强烈建议读一读），里面用到的就是贝叶斯方法，这里我们不打算复述他写的文章，而是简要地[将其核心思想介绍一下]。 #[[spell correct]]
    - 首先，我们需要询问的是：“**问题是什么？**”
        - 问题是我们看到[用户输入了][一个不在字典中的单词]，我们需要去猜测：“[这个家伙到底][真正想输入的单词]是什么呢？”用刚才我们[形式化的语言]来叙述就是，我们需要求：
**P(****我们猜测他想输入的单词 | 他实际输入的单词)**
这个概率。并找出那个[使得这个概率最大]的猜测单词。显然，我们的猜测[未必是唯一的]，就像前面举的那个[自然语言的歧义性]的例子一样；这里，比如用户输入： thew ，那么他到底是想输入 the ，还是想输入 thaw ？到底哪个猜测[可能性更大]呢？
        - 幸运的是我们可以用贝叶斯公式来直接出它们各自的概率，我们不妨将我们的[多个猜测]记为 h1 h2 .. （ h 代表 hypothesis），它们都属于[一个有限且离散的猜测空间] H （单词总共就那么多而已），将用户实际输入的单词记为 D （ D 代表 Data ，即观测数据），于是
**P(****我们的猜测1 | 他实际输入的单词)**
        - 运用一次贝叶斯公式，我们得到：
**P(h | D) = P(h) * P(D | h) / P(D)**
            - 对于不同的具体猜测 h1 h2 h3 .. ，P(D) 都是一样的，所以在比较 P(h1 | D) 和 P(h2 | D) 的时候我们可以忽略这个常数。即我们只需要知道：
P(h | D) ∝ P(h) * P(D | h) （注：那个符号的意思是“正比例于”，不是无穷大，注意符号右端是[有一个小缺口]的。）
            - 这个式子的[抽象含义]是：对于给定观测数据，一个猜测是好是坏，取决于“这个猜测本身[独立的可能性大小]（先验概率，[Prior]([[prior probability]]) ）”和“这个猜测[生成我们观测到的数据]的可能性大小”（似然，[[likelihood]] ）的乘积。具体到我们的那个 thew 例子上，含义就是，用户实际是想输入 the 的可能性大小取决于 the 本身在[词汇表]中[被使用的可能性]（频繁程度）大小（先验概率）和 想打 the 却打成 thew 的可能性大小（似然）的乘积。
        - 下面的事情就很简单了，对于我们[猜测为可能的]每个单词计算一下 P(h) * P(D | h) 这个值，然后取最大的，得到的就是最靠谱的猜测。
    - **一点注记**：Norvig 的[拼写纠正器]里面只提取了[编辑距离为 2 以内的][所有已知单词]。这是为了避免去[遍历字典中每个单词]计算它们的 P(h) * P(D | h) ，但这种做法为了[节省时间][带来了一些误差]。但话说回来难道我们人类真的回去[遍历每个可能的单词]来计算他们的后验概率吗？不可能。实际上，根据认知神经科学的观点，我们首先根据[错误的单词][做一个 bottom-up 的关联提取]，提取出[有可能是实际单词的]那些[候选单词]，这个提取过程就是所谓的[基于内容的提取]，可以根据错误单词的一些[模式片段][提取出有限的一组候选]，非常快地[缩小搜索空间]（比如我输入 explaination ，单词里面就[有充分的信息]使得我们的大脑[在常数时间内][把可能性 narrow down 到] explanation 这个单词上，至于具体是[根据哪些线索]——如音节——来提取，又是如何在[生物神经网络]中[实现这个提取机制的]，目前还是一个没有弄清的领域）。然后，我们对[这有限的几个猜测][做一个 top-down 的预测]，看看到底哪个对于[观测数据]（即错误单词）的[预测效力]最好，而[如何衡量预测效率]则就是用贝叶斯公式里面的那个 P(h) * P(D | h) 了——虽然我们很可能使用了[一些启发法来简化计算](http://www.douban.com/subject/1599035/)。后面我们还会提到这样的 bottom-up 的关联提取。
220824-21:25
