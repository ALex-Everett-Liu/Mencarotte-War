- 概率论只不过是把常识[用数学公式表达了出来]。
——拉普拉斯
- ## **0. 前言**
    - 这是一篇关于[贝叶斯方法]([[Bayesian method]])的科普文，我会尽量少用公式，多用平白的语言叙述，多举实际例子。[更严格的公式和计算][我会在相应的地方注明参考资料]。贝叶斯方法被证明是非常 general 且强大的[推理框架]，文中你会看到很多有趣的应用。
- ## **1. 历史**
    - 托马斯·贝叶斯（Thomas Bayes）同学的详细生平在[这里](http://en.wikipedia.org/wiki/Thomas_Bayes)。以下摘一段 wikipedia 上的简介：
        - 所谓的贝叶斯方法源于[他生前]为解决一个“逆概”问题写的一篇文章，而这篇文章是在他死后[才由他的一位朋友发表出来的]。在贝叶斯写这篇文章之前，人们已经能够计算“正向概率”，如“假设袋子里面有N个白球，M个黑球，你伸手进去摸一把，摸出黑球的概率是多大”。而一个自然而然的问题是反过来：“[如果我们事先并不知道][袋子里面黑白球的比例]，而是[闭着眼睛摸出一个]（或好几个）球，观察这些取出来的球的颜色之后，那么我们可以就此对袋子里面的黑白球的比例[作出什么样的推测]”。这个问题，就是所谓的[逆概问题]([[inverse probability]])。
    - 实际上，贝叶斯当时的论文只是对这个问题的[一个直接的求解尝试]，[并不清楚他当时是不是已经意识到][这里面包含着的深刻的思想]。然而后来，贝叶斯方法席卷了概率论，并将应用[延伸到各个问题领域]，[所有需要作出概率预测的地方][都可以见到贝叶斯方法的影子]，特别地，贝叶斯是[机器学习]([[machine learning]])的核心方法之一。这背后的深刻原因在于，[现实世界][本身就是不确定的]，[人类的观察能力][是有局限性的]（否则有很大一部分科学[就没有必要做了]——[设想我们能够直接观察到]电子的运行，还需要对原子模型[争吵不休]吗？），[我们日常所观察到的][只是事物表面上的结果]，沿用刚才那个袋子里面取球的比方，[我们往往只能知道][从里面取出来的球]是什么颜色，而[并不能直接看到][袋子里面实际的情况]。
        - 这个时候，我们就需要提供一个猜测（[[hypothesis]]，更为严格的说法是“假设”，这里用“猜测”更通俗易懂一点），所谓猜测，当然就是不确定的（[很可能有好多种乃至无数种]猜测[都能满足目前的观测]），**但也绝对不是[两眼一抹黑瞎蒙]——具体地说，我们需要做两件事情：1. 算出各种不同猜测的[可能性大小]。2. 算出[最靠谱的猜测]是什么。** 第一个就是[计算特定猜测的后验概率]，对于[连续的猜测空间]则是计算猜测的[概率密度函数]。第二个则是所谓的[模型比较]，模型比较如果[不考虑先验概率]的话就是[最大似然方法]([[maximum likelihood method]])。
220824-15:15
    - ### **1.1 一个例子：自然语言的二义性**
        - 下面举一个[自然语言的不确定性]的例子。当你看到这句话：
            - The girl saw the boy with a telescope.
            - 你对这句话的含义有什么猜测？平常人肯定会说：那个女孩拿望远镜看见了那个男孩（即你对[这个句子背后的实际语法结构]的猜测是：The girl saw-with-a-telescope the boy ）。然而，仔细一想，你会发现这个句子[完全可以解释成]：那个女孩看见了那个拿着望远镜的男孩（即：The girl saw the-boy-with-a-telescope ）。那为什么平常生活中[我们每个人都能够迅速地][对这种二义性进行消解]呢？[这背后到底隐藏着什么样的][思维法则]？我们留到后面解释。 #ambiguity
    - ### **1.2 贝叶斯公式**
        - 贝叶斯公式是怎么来的？
我们还是使用 wikipedia 上的一个例子：
            - 一所学校里面有 60% 的男生，40% 的女生。男生[总是穿长裤]，女生则[一半穿长裤一半穿裙子]。[有了这些信息之后][我们可以容易地计算]“[随机选取一个]学生，他（她）穿长裤的概率和穿裙子的概率是多大”，这个就是前面说的“正向概率”的计算。然而，假设你[走在校园中]，[迎面走来一个]穿长裤的学生（很不幸的是你高度近视，你只看得见他（她）穿的是否长裤，而无法确定他（她）的性别），你能够推断出他（她）是男生的概率是多大吗？
                - 一些[认知科学]的研究表明（《决策与判断》以及《[Rationality for Mortals](http://www.douban.com/subject/3199621/)》第12章：[小孩也可以解决]贝叶斯问题），我们对[形式化的贝叶斯问题]不擅长，但对于[以频率形式呈现的等价问题]却很擅长。在这里，我们不妨把问题重新叙述成：你在校园里面[随机游走]([[random walk]])，遇到了 N 个穿长裤的人（仍然假设你[无法直接观察到]他们的性别），问这 N 个人里面有多少个女生多少个男生。
                    - 你说，这还不简单：算出学校里面有多少穿长裤的，然后在这些人里面再算出有多少女生，不就行了？
                    - 我们来算一算：假设学校里面人的总数是 U 个。60% 的男生都穿长裤，于是我们得到了 U * P(Boy) * P(Pants|Boy) 个穿长裤的（男生）（其中 P(Boy) 是男生的概率 = 60%，这里可以简单的理解为男生的比例；P(Pants|Boy) 是条件概率，即在 Boy 这个条件下穿长裤的概率是多大，这里是 100% ，因为所有男生都穿长裤）。40% 的女生里面又有一半（50%）是穿长裤的，于是我们又得到了 U * P(Girl) * P(Pants|Girl) 个穿长裤的（女生）。加起来一共是 U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl) 个穿长裤的，其中有 U * P(Girl) * P(Pants|Girl) 个女生。两者一比就是你要求的答案。
                        - 下面我们把这个答案形式化一下：我们要求的是 P(Girl|Pants) （穿长裤的人里面有多少女生），我们计算的结果是 U * P(Girl) * P(Pants|Girl) / [U * P(Boy) * P(Pants|Boy) + U * P(Girl) * P(Pants|Girl)] 。容易发现这里校园内人的总数是无关的，可以消去。于是得到
**P(Girl|Pants) = P(Girl) * P(Pants|Girl) / [P(Boy) * P(Pants|Boy) + P(Girl) * P(Pants|Girl)]**
                            - 注意，如果把上式收缩起来，分母其实就是 P(Pants) ，分子其实就是 P(Pants, Girl) 。而这个比例很自然地就读作：在穿长裤的人（ P(Pants) ）里面有多少（穿长裤）的女孩（ P(Pants, Girl) ）。
                            - 上式中的 Pants 和 Boy/Girl 可以指代一切东西，所以其一般形式就是：
**P(B|A) = P(A|B) * P(B) / [P(A|B) * P(B) + P(A|~B) * P(~B) ]**
                                - 收缩起来就是：
**P(B|A) = P(AB) / P(A)**
                                - 其实这个就等于：
**P(B|A) * P(A) = P(AB)**
                                - 难怪拉普拉斯说**概率论只是把常识用数学公式表达了出来**。
然而，后面我们会逐渐发现，[看似这么平凡的]贝叶斯公式，[背后却隐含着][非常深刻的原理]。
- ## **2. 拼写纠正**
    - 经典著作《人工智能：现代方法》的作者之一 Peter Norvig 曾经写过一篇介绍如何写一个[拼写检查/纠正器]的文章（原文在[这里](http://norvig.com/spell-correct.html)，徐宥的翻译版在[这里](http://blog.youxu.info/spell-correct.html)，这篇文章很深入浅出，强烈建议读一读），里面用到的就是贝叶斯方法，这里我们不打算复述他写的文章，而是简要地[将其核心思想介绍一下]。 #[[spell correct]]
    - 首先，我们需要询问的是：“**问题是什么？**”
        - 问题是我们看到[用户输入了][一个不在字典中的单词]，我们需要去猜测：“[这个家伙到底][真正想输入的单词]是什么呢？”用刚才我们[形式化的语言]来叙述就是，我们需要求：
**P(****我们猜测他想输入的单词 | 他实际输入的单词)**
这个概率。并找出那个[使得这个概率最大]的猜测单词。显然，我们的猜测[未必是唯一的]，就像前面举的那个[自然语言的歧义性]的例子一样；这里，比如用户输入： thew ，那么他到底是想输入 the ，还是想输入 thaw ？到底哪个猜测[可能性更大]呢？
        - 幸运的是我们可以用贝叶斯公式来直接出它们各自的概率，我们不妨将我们的[多个猜测]记为 h1 h2 .. （ h 代表 hypothesis），它们都属于[一个有限且离散的猜测空间] H （单词总共就那么多而已），将用户实际输入的单词记为 D （ D 代表 Data ，即观测数据），于是
**P(****我们的猜测1 | 他实际输入的单词)**
        - 运用一次贝叶斯公式，我们得到：
**P(h | D) = P(h) * P(D | h) / P(D)**
            - 对于不同的具体猜测 h1 h2 h3 .. ，P(D) 都是一样的，所以在比较 P(h1 | D) 和 P(h2 | D) 的时候我们可以忽略这个常数。即我们只需要知道：
P(h | D) ∝ P(h) * P(D | h) （注：那个符号的意思是“正比例于”，不是无穷大，注意符号右端是[有一个小缺口]的。）
            - 这个式子的[抽象含义]是：对于给定观测数据，一个猜测是好是坏，取决于“这个猜测本身[独立的可能性大小]（先验概率，[Prior]([[prior probability]]) ）”和“这个猜测[生成我们观测到的数据]的可能性大小”（似然，[[likelihood]] ）的乘积。具体到我们的那个 thew 例子上，含义就是，用户实际是想输入 the 的可能性大小取决于 the 本身在[词汇表]中[被使用的可能性]（频繁程度）大小（先验概率）和 想打 the 却打成 thew 的可能性大小（似然）的乘积。
        - 下面的事情就很简单了，对于我们[猜测为可能的]每个单词计算一下 P(h) * P(D | h) 这个值，然后取最大的，得到的就是最靠谱的猜测。
    - **一点注记**：Norvig 的[拼写纠正器]里面只提取了[编辑距离为 2 以内的][所有已知单词]。这是为了避免去[遍历字典中每个单词]计算它们的 P(h) * P(D | h) ，但这种做法为了[节省时间][带来了一些误差]。但话说回来难道我们人类真的回去[遍历每个可能的单词]来计算他们的后验概率吗？不可能。实际上，根据认知神经科学的观点，我们首先根据[错误的单词][做一个 bottom-up 的关联提取]，提取出[有可能是实际单词的]那些[候选单词]，这个提取过程就是所谓的[基于内容的提取]，可以根据错误单词的一些[模式片段][提取出有限的一组候选]，非常快地[缩小搜索空间]（比如我输入 explaination ，单词里面就[有充分的信息]使得我们的大脑[在常数时间内][把可能性 narrow down 到] explanation 这个单词上，至于具体是[根据哪些线索]——如音节——来提取，又是如何在[生物神经网络]中[实现这个提取机制的]，目前还是一个没有弄清的领域）。然后，我们对[这有限的几个猜测][做一个 top-down 的预测]，看看到底哪个对于[观测数据]（即错误单词）的[预测效力]最好，而[如何衡量预测效率]则就是用贝叶斯公式里面的那个 P(h) * P(D | h) 了——虽然我们很可能使用了[一些启发法来简化计算](http://www.douban.com/subject/1599035/)。后面我们还会提到这样的 bottom-up 的关联提取。
220824-21:25
- ## **3. 模型比较与奥卡姆剃刀**
    - ### **3.1 ****再访拼写纠正**
        - 介绍了贝叶斯拼写纠正之后，接下来的[一个自然而然的问题]就来了：“**为什么？**”为什么要用贝叶斯公式？为什么贝叶斯公式在这里可以用？我们可以[很容易地领会]为什么贝叶斯公式用在前面介绍的那个男生女生长裤裙子的问题里是正确的。但为什么这里？
            - 为了回答这个问题，[一个常见的思路]就是想想：**非得这样吗？**因为[如果你想到了另一种做法]并且[证明了它也是靠谱的]，那么将它与现在这个一比较，也许就能得出很有价值的信息。那么对于拼写纠错问题[你能想到其他方案吗]？
        - 不管怎样，一个最常见的替代方案就是，选择离 thew 的[编辑距离]([[edit distance]])最近的。然而 the 和 thaw 离 thew 的编辑距离都是 1 。这可咋办捏？你说，不慌，那还是好办。我们就看到底[哪个更可能][被错打为] thew 就是了。我们注意到字母 e 和字母 w 在键盘上离得很紧，[无名指一抽筋][就不小心多打出一个] w 来，the 就变成 thew 了。而另一方面 thaw 被错打成 thew 的可能性就相对小一点，因为 e 和 a [离得较远]而且使用的指头[相差一个指头]（一个是中指一个是小指，不像 e 和 w 使用的指头靠在一块——[神经科学的证据表明][紧邻的身体设施之间][容易串位]）。OK，很好，因为你现在已经是在用[最大似然方法]了，或者直白一点，你就是在计算那个使得 P(D | h) 最大的 h 。
            - 而贝叶斯方法计算的是什么？是 P(h) * P(D | h) 。多出来了一个 P(h) 。我们刚才说了，这个多出来的 P(h) 是特定猜测的先验概率。为什么要掺和进一个先验概率？刚才说的那个最大似然不是挺好么？[很雄辩地指出了] the 是更靠谱的猜测。有什么问题呢？既然这样，我们就从给最大似然[找茬]开始吧——我们假设两者的[似然程度]是一样或非常相近，这样不就难以区分哪个猜测更靠谱了吗？比如用户输入tlp ，那到底是 top 还是 tip ？（这个例子不怎么好，因为 top 和 tip 的词频可能仍然是接近的，但一时想不到好的英文单词的例子，我们不妨就假设 top 比 tip 常见许多吧，这个假设[并不影响问题的本质]。）这个时候，当最大似然[不能作出决定性的判断]时，先验概率[就可以插手进来][给出指示]——“既然你无法决定，那么我告诉你，一般来说 top 出现的程度要高许多，所以更可能他想打的是 top ”）。
            - 以上只是最大似然的一个问题，即并不能提供[决策的全部信息]。
